from dataclasses import dataclass
from typing import Optional, Tuple, List
from multiprocessing.pool import Pool
from logzero import logger
from BITS.seq.io import db_to_n_reads
from BITS.util.io import save_pickle
from BITS.util.scheduler import Scheduler, run_distribute
from .core import find_units
from ..type import TRRead


@dataclass(eq=False)
class DatrufRunner:
    """Find unit sequences of tahndem repeats using the result of datander.

    usage:
      > r = DatrufRunner("READS.db", "TAN.READS.las", n_core=10)
      > r.run()

    positional arguments:
      @ db_fname  : DAZZ_DB file name.
      @ las_fname : `TAN.*.las` file generated by datander.

    optional arguments:
      @ max_cv       : Exclude a set of units cut from a tandem repeat if it has
                       a coefficient of variation greater than this value.
                       CV represents how a self alignment is wobbling.
      @ scheduler    : `BITS.util.scheduler.Scheduler` object.
      @ n_distribute : Number of jobs to be distributed.
      @ n_core       : Number of cores used in each job.
                       `n_distribute * n_core` cores are used in total.
      @ out_fname    : Output pickle file name.
      @ tmp_dname    : Relative path to a directory for intermediate files.
    """
    db_fname: str
    las_fname: str
    max_cv: float = 0.1
    scheduler: Optional[Scheduler] = None
    n_distribute: int = 1
    n_core: int = 1
    out_fname: str = "tr_reads.pkl"
    tmp_dname: str = "datruf"

    def __post_init__(self):
        assert self.n_distribute == 1 or self.scheduler is not None, \
            "`scheduler` is required when `n_distribute` > 1"

    def run(self):
        n_reads = db_to_n_reads(self.db_fname)
        # Without job scheduler
        if self.scheduler is None:
            save_pickle(find_units_multi([(1, n_reads)],
                                         self.db_fname,
                                         self.las_fname,
                                         self.max_cv,
                                         self.n_core),
                        self.out_fname)

        # With job scheduler
        n_split = self.n_distribute * self.n_core
        unit_n = -(-n_reads // n_split)
        args = [(1 + i * unit_n,
                 min([1 + (i + 1) * unit_n - 1, n_reads]))
                for i in range(n_split)]
        logger.info(f"args={args}")
        save_pickle(run_distribute(func=find_units_multi,
                                   args=args,
                                   shared_args=dict(db_fname=self.db_fname,
                                                    las_fname=self.las_fname,
                                                    max_cv=self.max_cv),
                                   scheduler=self.scheduler,
                                   n_distribute=self.n_distribute,
                                   n_core=self.n_core,
                                   tmp_dname=self.tmp_dname,
                                   job_name="datruf",
                                   out_fname=self.out_fname),
                    self.out_fname)


def find_units_multi(args: List[Tuple[int, int]],
                     db_fname: str,
                     las_fname: str,
                     max_cv: float,
                     n_core: int) -> List[TRRead]:
    tr_reads = []
    with Pool(n_core) as pool:
        for ret in pool.starmap(find_units,
                                [(start_dbid,
                                  end_dbid,
                                  db_fname,
                                  las_fname,
                                  max_cv)
                                 for start_dbid, end_dbid in args]):
            tr_reads += ret
    return tr_reads
